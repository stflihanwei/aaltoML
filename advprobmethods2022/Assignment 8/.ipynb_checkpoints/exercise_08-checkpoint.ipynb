{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb8578c9b4e4aaa41894034060ca28db",
     "grade": false,
     "grade_id": "cell-7e3f1e2b3d7409fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2022)\n",
    "\n",
    "Pekka Marttinen, Prayag Tiwari, Vishnu Raj, Tianyu Cui, Yogesh Kumar, Antti Pöllänen, Louis Filstroff, Alex Aushev, Zheyang Shen, Nikitin Alexander , Sebastiaan De Peuter.\n",
    "\n",
    "## Exercise 8, due on Tuesday March 29 at 23:50.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: Minimize KL divergence using PyTorch\n",
    "2. Problem 2: VB for a factor analysis model (1/2)\n",
    "3. Problem 3: VB for a factor analysis model (2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48aefe82a5afe18f9185bfb71bbfb637",
     "grade": false,
     "grade_id": "cell-8f76ca03c69cb4d1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 1: Minimize KL divergence using PyTorch\n",
    "PyTorch is a powerful auto-differentiation framework that allows us to do any optimization, as long as we can define the objective function and corresponding optimization variables. It has been widely used for Bayesian deep learning. In this exercise, we will study how to use PyTorch to fit a Gaussian distribution to a known Mixture of Gaussian by minimizing their KL divergence, and compare the difference between the forward and reverse form of the KL.\n",
    "\n",
    "Recall that the KL divergence between two distributions $q(x)$ and $p(x)$ is defined as:\n",
    "\n",
    "$$\\text{KL}[q(x)|p(x)]=\\int q(x)\\log\\frac{q(x)}{p(x)}dx.$$\n",
    "\n",
    "This is typically called the **Reverse KL** which we have used before in the course (like in Variational Bayes). If the probability density functions of $q(x)$ and $p(x)$ are known, and we can get samples from $q(x)$, an unbiased estimator of KL divergence is:\n",
    "$$\\text{KL}[q(x)|p(x)]\\approx\\log\\frac{q(x_i)}{p(x_i)}=\\log q(x_i)-\\log p(x_i),$$\n",
    "where $x_i\\sim q(x)$. We will use above estimator for this exercise.\n",
    "\n",
    "There is also a **Forward KL**: $\\text{KL}[p(x)|q(x)]$ defined as:\n",
    "\n",
    "$$\\text{KL}[p(x)|q(x)]=\\int p(x)\\log\\frac{p(x)}{q(x)}dx,$$\n",
    "\n",
    "which is used in other inference algorithms such as Expectration Propogation which is not within the scope of this course.\n",
    "\n",
    "Let $p(x \\mid \\pi) = \\pi \\mathcal{N}(0,1)+(1-\\pi)\\mathcal{N}(8,1)$ where $\\pi\\sim\\text{Bernoulli}(0.4)$ be the true mixture distribution which we want to fit using a Gaussian $q(x; \\mu, \\sigma)$. We want to estimate $\\mu$ and $\\sigma$ using both the forwared and reverse KL.\n",
    "\n",
    "Complete the template below with the relevant code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d052f67014581203e59c5bf9d58823cc",
     "grade": false,
     "grade_id": "cell-33040ac0aa3bb106",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as Dis\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Gaussian:\n",
    "    \"\"\"\n",
    "    This represents q(x) \n",
    "    Gaussian distribution is parametrized by mean (mu) and standard deviation. The standard deviation is \n",
    "    parametrized as sigma = log(1 + exp(rho)) to make it positive all the time. A sample from the distribution\n",
    "    can be obtained by first sampling from a unit Gaussian, shifting the samples by the mean and scaling by the \n",
    "    standard deviation: w = mu + log(1 + exp(rho)) * epsilon\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, rho):\n",
    "        self.mean = mu\n",
    "        self.rho = rho\n",
    "\n",
    "    @property\n",
    "    def std_dev(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "\n",
    "    def sample(self, num_samples = 1):\n",
    "        # Sample num_samples data points from Gaussian distribution\n",
    "        # Return a tensor contains all the samples \n",
    "        \n",
    "        # Sample num_samples datapoints from N(0,1) \n",
    "        epsilon = Dis.Normal(0,1).sample([num_samples])\n",
    "        \n",
    "        # Scale and shift epsilon\n",
    "        # samples = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def logprob(self, samples):\n",
    "        # Compute the log probability of each sample under Gaussian distribution\n",
    "        # Return a tensor containing the log probability of all samples        \n",
    "        \n",
    "        # logp = ? # EXERCISE\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return logp\n",
    "    \n",
    "class MoG:\n",
    "    \"\"\"\n",
    "    This represents p(x).\n",
    "    In this example, mixture of two Gaussian distribution is constructed by 2 Gaussian distributions \n",
    "    N(0,2) and N(8,1), and each datapoint is from N(0,2) with probability p = 0.4 and from N(8,1) with \n",
    "    probability 0.6.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu_1=0., sigma_1=1., mu_2=8., sigma_2=1., prob = 0.4):\n",
    "        self.mean_1 = torch.tensor(mu_1)\n",
    "        self.sigma_1 = torch.tensor(sigma_1)\n",
    "        self.mean_2 = torch.tensor(mu_2)\n",
    "        self.sigma_2 = torch.tensor(sigma_2)\n",
    "        self.prob = torch.tensor(prob)\n",
    "\n",
    "    def sample(self, num_samples = 1):\n",
    "        # Sample num_samples data points from MoG distribution\n",
    "        # Return a tensor contains all the samples\n",
    "        \n",
    "        # sample from N(0, 2)\n",
    "        # sample form N(8, 1)\n",
    "        # sample from Bern(0.4)\n",
    "        # Combine the three to from a sample form mixture\n",
    "        # sample_gaussian_1 = ? # EXERCISE\n",
    "        # sample_gaussian_2 = ? # EXERCISE\n",
    "        # sample_bernoulli = ? # EXERCISE\n",
    "        # samples  = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def logprob(self, samples):\n",
    "        \n",
    "        # Compute the log probability of each sample under the MoG distribution\n",
    "        # Return a tensor containing the log probability of all samples\n",
    "        \n",
    "        # logp = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return logp    \n",
    "\n",
    "class KL_divergence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KL_divergence, self).__init__()\n",
    "        # define the mean and standard deviation as parameters, and initialization\n",
    "        self.mu = nn.Parameter(torch.Tensor(1).uniform_(-2., 12.))\n",
    "        self.rho = nn.Parameter(torch.Tensor(1).uniform_(1.0, 5.0))\n",
    "        \n",
    "        self.gaussian = Gaussian(self.mu, self.rho)\n",
    "        self.mog = MoG()\n",
    "    \n",
    "    def compute_forwardKL(self):\n",
    "        num_samples = torch.tensor(1000)\n",
    "        \n",
    "        # compute the forward KL divergence between p and q of num_samples data points\n",
    "        # Return the estimated forward KL divergence\n",
    "        \n",
    "        \n",
    "        # sample form MoG\n",
    "        # compute forware KL \n",
    "        \n",
    "        # samples = ? # EXERCISE\n",
    "        # fkl = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return fkl\n",
    "    \n",
    "    def compute_reverseKL(self):\n",
    "        num_samples = torch.tensor(1000)\n",
    "        # compute the reverse KL divergence between p and q with num_samples data points\n",
    "        # Return the estimated reverse KL divergence\n",
    "        \n",
    "        # sample form Gaussian\n",
    "        # compute reverse KL \n",
    "        \n",
    "        # samples = ? # EXERCISE\n",
    "        # rkl = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return rkl\n",
    "\n",
    "# Optimize the KL by using gradient descent\n",
    "def optimization(kl, forward = False, learning_rate = 0.1, num_epoch = 1000):\n",
    "    parameters = set(kl.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr = learning_rate, eps=1e-3)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        if forward:\n",
    "            loss = kl.compute_forwardKL()\n",
    "        else:\n",
    "            loss = kl.compute_reverseKL()\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch % 100) == 0:\n",
    "            print('EPOACH %d: KL: %.4f.'% (epoch+1, loss))\n",
    "\n",
    "print('Optimizing reverse KL')\n",
    "torch.manual_seed(0)\n",
    "kl_reverse = KL_divergence()\n",
    "optimization(kl_reverse, forward = False)\n",
    "Gaussian_reverse= kl_reverse.gaussian\n",
    "\n",
    "print('Optimizing forward KL')\n",
    "kl_forward = KL_divergence()\n",
    "optimization(kl_forward, forward= True)\n",
    "Gaussian_forward = kl_forward.gaussian\n",
    "\n",
    "# Plot the pdf of Gaussian fitted from forward KL and reverse KL, and also the ground truth pdf from MoG\n",
    "x_plot = torch.linspace(-5., 15., 1000)\n",
    "density_mog = torch.exp(kl_forward.mog.logprob(x_plot)).detach().numpy()\n",
    "density_Gaussian_forward = torch.exp(Gaussian_forward.logprob(x_plot)).detach().numpy()\n",
    "density_Gaussian_reverse = torch.exp(Gaussian_reverse.logprob(x_plot)).detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_plot, density_mog)\n",
    "ax.plot(x_plot, density_Gaussian_forward)\n",
    "ax.plot(x_plot, density_Gaussian_reverse)\n",
    "\n",
    "ax.legend(('MoG Density','Forward KL', 'Reverse KL'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9b26bff6d2b8ec275fbb86082cee644",
     "grade": false,
     "grade_id": "cell-c9f75665b42eb29f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: VB for a factor analysis model (1/2)\n",
    "\n",
    "The data set consists of $D$-dimensional vectors $\\mathbf{x}_{n}\\in \\mathbb{R}^{D},$ for $n=1,\\ldots,N$. We model the data using factor analysis with $K$-dimensional factors $\\mathbf{z}_{n}\\in\\mathbb{R}^{K}$. In detail, the\n",
    "model is specified as follows:\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_{n} &  \\sim\\mathcal{N}_{D}(\\mathbf{Wz}_{n},\\text{diag}\n",
    "(\\mathbf{\\psi})^{-1}),\\quad n=1,\\ldots,N,\\\\\n",
    "\\psi_{d} &  \\sim\\text{Gamma}(a,b),\\quad d=1,\\ldots,D,\\\\\n",
    "\\mathbf{w}_{d} &  \\sim\\mathcal{N}_{K}(\\mathbf{0,}\\alpha\\mathbf{I}),\\quad\n",
    "d=1,\\ldots,D,\\\\\n",
    "\\mathbf{z}_{n} &  \\sim\\mathcal{N}_{K}(\\mathbf{0,I}),\\quad n=1,\\ldots,N.\n",
    "\\end{align*}\n",
    "Here, $\\mathbf{W}$ is a $D\\times K$ factor loading matrix and $\\mathbf{w}_{d}$ is the $d$th row of $\\mathbf{W}$ written as a column vector. Parameter $\\psi_{d}^{-1}$ is the variance for the $d$th dimension in the observed data\n",
    "and diag$(\\psi)$ denotes a diagonal matrix with elements $\\mathbf{\\psi} =(\\psi_{1},\\ldots,\\psi_{D})^{T}$ on the diagonal.\n",
    "\n",
    "We approximate the posterior $p(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W} |\\mathbf{X})$ using the mean-field approximation: \n",
    "\n",
    "$$ \n",
    "q(\\Theta)=\\prod_{d=1}^{D}q(\\mathbf{w}_{d})\\prod_{n=1}^{N}q(\\mathbf{z}_{n})\\prod_{d=1}^{D}q(\\psi_{d}).\n",
    "$$\n",
    "\n",
    "\n",
    "__1__ Write the logarithm of the joint distribution, $\\log p(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W},\\mathbf{X})$.\n",
    "\n",
    "__2__ Remove from the logarithm of the joint distribution all terms that do not depend on $\\mathbf{z}_{n}$.\n",
    "\n",
    "__3__ Show that the updated factor $q(\\mathbf{z}_{n})$ is equal to\n",
    "\n",
    "$$\n",
    "q(\\mathbf{z}_{n})=\\mathcal{N}_{K}(\\mathbf{\\mu}_{n},\\mathbf{K}_{n}),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{K}_{n} &  =\\left[  I+\\sum_{d=1}^{D}\\left\\langle \\psi_{d}\\right\\rangle\n",
    "\\left\\langle \\mathbf{w}_{d}\\mathbf{w}_{d}^{T}\\right\\rangle \\right]  ^{-1}\n",
    "\\quad\\text{and}\\\\\n",
    "\\mathbf{\\mu}_{n} &  =\\mathbf{K}_{n}\\left\\langle \\mathbf{W}^{T}\\right\\rangle\n",
    "\\text{diag}(\\left\\langle \\mathbf{\\psi}\\right\\rangle )\\mathbf{x}_{n}.\n",
    "\\end{align*}\n",
    "\n",
    "Here $\\left\\langle \\mathbf{\\cdot}\\right\\rangle $ is used as a shorthand for the expectation of a variable with respect to its factor, e.g., $\\left\\langle \\mathbf{\\psi}\\right\\rangle =\\mathbb{E}_{q(\\mathbf{\\psi})}[\\mathbf{\\psi}]$ etc.\n",
    "\n",
    "__Hint 1:__ Try to write the log joint as \n",
    "\n",
    "$$\n",
    "-\\frac{1}{2}\\mathbf{z}_{n}^{T}\\mathbf{Az}_{n}+\\mathbf{b}^{T}\\mathbf{z}_{n}\n",
    "$$\n",
    "\n",
    "for some $\\mathbf{A}$ and $\\mathbf{b}$, after which you can apply the 'completing the square' technique.\n",
    "\n",
    "__Hint 2:__ Suppose $\\mathbf{A}$ is an $N\\times M$ matrix. Further suppose that $\\mathbf{D}$ is an $N\\times N$ diagonal matrix, $\\mathbf{D} =$diag$(d_{1},\\ldots,d_{N})$. Then $\\mathbf{A}^{T}\\mathbf{DA}$ can be written\n",
    "as\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^{T}\\mathbf{DA=}\\sum_{n=1}^{N}d_{n}\\mathbf{a}_{n}\\mathbf{a}_{n}^{T},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{a}_{n}$ is the $n$th row of $\\mathbf{A}$ written as a column vector.\n",
    "\n",
    "__Hint 3:__ Recall that expectation is a linear operator, i.e. $\\mathbb{E}(aX+bY)=a\\mathbb{E}(X)+b\\mathbb{E}(Y)$. Further, if some random variables $A$ and $B$ are independent, then $\\mathbb{E}_{q(A)q(B)} (AB)=\\mathbb{E}_{q(A)}(A)\\mathbb{E}_{q(B)}(B)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 2 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "337e6c54afe83f1330cba6cf779db1aa",
     "grade": false,
     "grade_id": "cell-fd4f81c2e2c0650f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 3: VB for a factor analysis model (2/2)\n",
    "For the factor analysis model considered in Problem 2, derive the update for factor $q(\\mathbf{w}_{d})$. The updated factor should be given in terms of the following expectations: $\\left\\langle \\psi_{d}\\right\\rangle ,\\left\\langle \\mathbf{z}_{n}\\right\\rangle ,\\left\\langle \\mathbf{z}_{n}\\mathbf{z}_{n}^{T}\\right\\rangle $, which have been calculated using the current values of the other factors for all $d,n$.\n",
    "\n",
    "__Hint__: A multivariate Gaussian with a diagonal covariance can be expressed as a product of independent univariate Gaussians, which allows you to simplify the formulas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 3 here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
