{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e92fb3725fc23e8bc2b845a67a835b3",
     "grade": false,
     "grade_id": "cell-5b335005bb36ae92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2022)\n",
    "\n",
    "Pekka Marttinen, Prayag Tiwari, Vishnu Raj, Tianyu Cui, Yogesh Kumar, Antti Pöllänen, Louis Filstroff, Alex Aushev, Zheyang Shen, Nikitin Alexander , Sebastiaan De Peuter.\n",
    "\n",
    "## Exercise 5, due on Tuesday March 8 at 23:50.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: EM for missing observations\n",
    "2. Problem 2: Extension of 'simple example' from the lecture\n",
    "3. Problem 3: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18c12b98afa6a333b6b4717029202b7d",
     "grade": false,
     "grade_id": "cell-298bb2ed1de6d806",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 1: EM for missing observations\n",
    "Suppose random variables $X_{i}$ follow a bivariate normal distribution $X_{i}\\sim \\mathcal{N}_{2}(0,\\Sigma)$, where\n",
    "$ \\Sigma = \\begin{bmatrix} 1 & \\rho\\\\ \\rho & 1 \\end{bmatrix} $.\n",
    "\n",
    "Suppose further that we have observations on $X_{1}=(X_{11},X_{12})^{T}$, $X_{2}=(X_{21},X_{22})^{T}$ and $X_{3}=(X_{31},X_{32})^{T}$, such that\n",
    "$X_{1}$ and $X_{3}$ are fully observed, and from $X_{2}$ we have observed only\n",
    "the second coordinate. Thus, our data matrix can be written as\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12}\\\\\n",
    "? & x_{22}\\\\\n",
    "x_{31} & x_{32}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "where the rows correspond to the transposed observations $\\mathbf{x}_{1}^{T},\\mathbf{x}_{2}^{T},\\mathbf{x}_{3}^{T}$. Suppose we want to learn the unknown parameter $\\rho$ using the EM-algorithm. Denote the missing observation by $Z$ and derive the E-step of the algorithm, i.e., __(a)__ write the complete data log-likelihood $\\ell(\\rho)$, __(b)__ compute the posterior distribution of the missing observation, given the observed variables and current estimate for $\\rho$, and __(c)__ evaluate the expectation of $\\ell(\\rho)$ with respect to the posterior distribution of the missing observations.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "1. In general, for $X \\sim \\mathcal{N}_2(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, where $X=(X_1, X_2)^{T}$, $\\boldsymbol{\\mu}=(\\mu_1, \\mu_2)^{T}$ and $\\boldsymbol{\\Sigma} = \\begin{pmatrix} \n",
    "            \\sigma_1^{2} & \\rho\\sigma_{1}\\sigma_{2} \\\\ \n",
    "            \\rho\\sigma_{1}\\sigma_{2} & \\sigma_2^{2} \n",
    "            \\end{pmatrix}$, \n",
    "we have \n",
    "$$ X_1 \\mid X_2 = x_2 \\sim \\mathcal{N}\\left(\\mu_1 + \\frac{\\sigma_1}{\\sigma_2}\\rho(x_2-\\mu_2), (1-\\rho^2)\\sigma_1^{2}\\right),$$  with $\\rho$ being the correlation coefficient.\n",
    "2. For evaluating the expectation of $\\ell(\\rho)$, you can make use of the following two rules: \n",
    "    - $\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2} = trace(\\boldsymbol{\\Sigma}^{-1}\\mathbf{x_2x_2^T}).$\n",
    "    - if $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ then $\\langle{X^2}\\rangle = \\mu^2 + \\sigma^2$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1cd36c403dde3a532a877a43ad92522",
     "grade": false,
     "grade_id": "cell-46bf29d7d4d92271",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: Extension of 'simple example' from the lecture\n",
    "Suppose that we have $N$ independent observations $x = ( x_1, \\dots, x_N )$ from a two-component mixture of univariate Gaussian distributions with unknown mixing co-efficients and unknown mean of the second component:\n",
    "$$ p(x_{n} \\mid \\theta,\\tau)=(1-\\tau)\\mathcal{N}(x_{n}|0,1)+\\tau\\mathcal{N}(x_{n} \\mid \\theta,1).$$\n",
    "\n",
    "**(a)** Write down the complete data log-likelihood and derive the EM-algorithm for learning the maximum likelihood estimates for $\\theta$ and $\\tau$. \n",
    "\n",
    "**(b)** Simulate some data from the model ($N = 100$ samples) with the true values of parameters $\\theta$ = 3 and $\\tau = 0.5$. Run your EM algorithm to see whether the learned parameters converge close to the true values (by e.g. just listing the estimates from a few iterations or plotting them). Use the code template below (after the answer cell) as a starting point. \n",
    "\n",
    "**HINT**: The E and M steps for simple example.pdf from the lecture material looks as follows\n",
    "```Python\n",
    "\t# E-step: compute the responsibilities r2 for component 2\n",
    "\tr1_unnorm = scipy.stats.norm.pdf(x, 0, 1)\n",
    "\tr2_unnorm = scipy.stats.norm.pdf(x, theta_0, 1)\n",
    "\tr2 = r2_unnorm / (r1_unnorm + r2_unnorm)\n",
    "\t\n",
    "\t# M-step: compute the parameter value that maximizes\n",
    "\t# the expectation of the complete-data log-likelihood.\n",
    "\ttheta[it] = sum(r2 * x) / sum(r2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 2(a) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b479fd715d72b0417e4f3ac4c0f8e914",
     "grade": false,
     "grade_id": "cell-1abac854e88e7dc1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# template for Problem 2(b)\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### Simulate data:\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "theta_true = 3\n",
    "tau_true = 0.5\n",
    "n_samples = 100\n",
    "\n",
    "x = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    # Sample from N(0,1) or N(theta_true,1)\n",
    "    if np.random.rand() < 1 - tau_true:\n",
    "        x[i] = np.random.normal(0, 1)\n",
    "    else:\n",
    "        x[i] = np.random.normal(theta_true, 1)\n",
    "\n",
    "\n",
    "### The EM algorithm:\n",
    "\n",
    "n_iter = 20\n",
    "theta = np.zeros(n_iter)\n",
    "tau = np.zeros(n_iter)\n",
    "\n",
    "# Initial guesses for theta and tau\n",
    "theta[0] = 1\n",
    "tau[0] = 0.1\n",
    "\n",
    "for it in range(1, n_iter):\n",
    "    # The current estimates for theta and tau,\n",
    "    # computed in the previous iteration\n",
    "    theta_0 = theta[it-1]\n",
    "    tau_0 = tau[it-1]\n",
    "\n",
    "    # E-step: compute the responsibilities r1 and r2\n",
    "    # r1 = ?\n",
    "    # r2 = ?\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # M-step: compute the parameter values that maximize\n",
    "    # the expectation of the complete-data log-likelihood.\n",
    "    # theta[it] = ?\n",
    "    # tau[it] = ?\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "\n",
    "# Print and plot the values of theta and tau in each iteration\n",
    "print(\"theta       tau\")\n",
    "for theta_i, tau_i in zip(theta, tau):\n",
    "    print(\"{0:.7f}  {1:.7f}\".format(theta_i, tau_i))\n",
    "\n",
    "plt.plot(range(n_iter), theta, label = 'theta')\n",
    "plt.plot(range(n_iter), tau, label = 'tau')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82083e7d021d449db017e6010251e7ad",
     "grade": false,
     "grade_id": "cell-482274cb8fbd6887",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Problem 3: PyTorch\n",
    "Go through the PyTorch tutorials in the three links and answer the questions given below\n",
    "\n",
    "1) What is PyTorch: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "\n",
    "2) Autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\n",
    "\n",
    "3) Linear regression with PyTorch: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/linear_regression/main.py\n",
    "\n",
    "__(a)__ What are PyTorch Tensors and how do you run a CPU tensor on GPU? \n",
    "\n",
    "\n",
    "__(b)__ What is Automatic differentiation and autograd? \n",
    "\n",
    "\n",
    "__(c)__ PyTorch constructs the computation graph dynamically as the operations are defined. In the 'linear regression with PyTorch' tutorial which line numbers indicates the completion of the computation graph, computation of the gradients and update of the weights, respectively? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 3 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
