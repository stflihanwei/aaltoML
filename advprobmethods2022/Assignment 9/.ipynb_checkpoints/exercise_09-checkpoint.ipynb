{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5WQVkXC6okwW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e08e86cf4f8a331c0a09c583e5201600",
     "grade": false,
     "grade_id": "cell-7cfcd7a9e55b757e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2022)\n",
    "\n",
    "Pekka Marttinen, Prayag Tiwari, Vishnu Raj, Tianyu Cui, Yogesh Kumar, Antti Pöllänen, Louis Filstroff, Alex Aushev, Zheyang Shen, Nikitin Alexander , Sebastiaan De Peuter.\n",
    "\n",
    "## Exercise 9, due on Tuesday April 7 at 23:50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LFGuZUGvokwc",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef6612bc212a494f84d418fec8ab4226",
     "grade": false,
     "grade_id": "cell-6d5142a7aa83daaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SVI for linear regression using PyTorch\n",
    "\n",
    "In this exercise, we will see how to use stochastic variational inference (especially the pathwise estimator) to solve linear regression problem using autograd in PyTorch.\n",
    "\n",
    "### Bayesian Linear Regression\n",
    "The model is defined as follows: \n",
    "\\begin{align*}\n",
    "y_i &  \\sim \\mathcal{N}(w_0 + w_1x_i, \\sigma_l^2), \\quad x_i \\in \\mathbb{R}, \\sigma_l=0.4, i=1,\\ldots,N\\\\\n",
    "\\mathbf{w} &  \\sim\\mathcal{N}(0, \\alpha^2I).\n",
    "\\end{align*}\n",
    "Note: The data noise is large because the true model used to generate the data is more complex to which we are going to fit a linear model. \n",
    "\n",
    "Given data $\\mathcal{D} = \\{ (x_i, y_i) \\}_{i=1}^{N}$, we are interested in the posterior distribution $p(\\mathbf{w}|\\mathcal{D})$ which we approximate using  mean-field approximation: $$ p(\\mathbf{w}|\\mathcal{D}) \\approx q(\\mathbf{w}) = \\prod_{d=0}^1 q(w_d) = \\prod_{d=0}^1 \\mathcal{N}(w_d | \\mu_d,\\sigma_d^2)$$\n",
    "That is, we model each $w_d$ as an independent Gaussian with mean $\\mu_d$ and $\\sigma_d^2$ and use SVI to optimize them such that: \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\lambda} & = \\text{argmin}_{\\lambda}\\text{KL}[q(\\mathbf{w})|p(\\mathbf{w} | \\mathcal{D})] \\\\\n",
    "&= \\text{argmin}_{\\lambda} \\underbrace{\\mathbb{E}_{q_{\\lambda}(\\mathbf{w})}\\left[-\\log p(\\mathcal{D}|\\mathbf{w})\\right] + \\text{KL}\\left[q(\\mathbf{w})| p(\\mathbf{w})\\right]}_{Loss = - ELBO}+c. \n",
    "\\end{align}\n",
    "Here, the variational parameters are denotd by $\\lambda = \\{ (\\mu_d, \\sigma_d), i = 0, 1 \\}$. The first term of the ELBO is the expected log likelihood, which will be estimated using pathwise estimator and the second term is the KL between the approximate posterior $q_{\\lambda}(\\mathbf{w})$ and the prior $p(\\mathbf{w})$ that can be derived analytically in this case. \n",
    "We will solve this problem in three steps given as three problem below. In the first two problems we derive the two terms of the Loss which, in problem 3 are implemented using the pathwise estimator in PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zSgoKuP9okwd",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58ab046cb7730fd315a3514b925aa6c7",
     "grade": false,
     "grade_id": "cell-542be279803115bd",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 1: Negative log-likelihood\n",
    "Write the negative log-likelihood (whose expectation is the first term in the Loss) as a scaled mean squared error. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAZTgSysokwd"
   },
   "source": [
    "Write your answer to Problem 1 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "18q4e3EPokwd",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7f119479d43e03f359b4aa66752f0c3",
     "grade": false,
     "grade_id": "cell-104b91beeb13e1bc",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: Derive KL Divergence \n",
    "\n",
    "Derive the analytic solution of $\\text{KL}[q_{\\lambda}(\\mathbf{w})|p(\\mathbf{w})]$. This will be required in Problem 3.\n",
    "\n",
    "__Hint:__ Given $\\mathbf{w}$ is a MVN with diagonal covarience and the mean-field approximation of $q_{\\lambda}(\\mathbf{w})$, the KL divergence for both the components of $\\mathbf{w} = (w_0, w_1)$ will have the same form. So this reduces to deriving the KL between two univariate Guassians.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t19i8DTcokwe"
   },
   "source": [
    "Write your answer to Problem 2 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "NealwNzWokwe",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "093ae8f996d3d93dc67c1436dbc3a63a",
     "grade": false,
     "grade_id": "cell-6f67b14f30cacdeb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 3: Pathwise Estimator in PyTorch\n",
    "Complete the code template below that implements the pathwise estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SzYNHUQokwe"
   },
   "outputs": [],
   "source": [
    "# Starter code for problem 3\n",
    "# We first simulate the data using following simulator to generate our training and test data: \n",
    "# $y_i=x_i+0.7\\sin(3x_i)+\\epsilon,$ where $\\epsilon\\sim\\mathcal{N}(0,0.16)$\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We define a function to generate the data according to the simulator\n",
    "def data_generation(num_data, interval):\n",
    "    x = np.random.rand(num_data,1) * (interval[1] - interval[0]) + interval[0]\n",
    "    e = np.random.randn(num_data,1) * 0.4\n",
    "    y =  x + 0.7 * np.sin(3 * x) + e\n",
    "    return torch.tensor(x, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "# Generate the 100 data points with x in [-3, 3] for training, validation, and test dataset.\n",
    "interval = [-3,3]\n",
    "num_data = 100\n",
    "x_train, y_train = data_generation(num_data, interval)\n",
    "x_val, y_val = data_generation(num_data, interval)\n",
    "x_test, y_test = data_generation(num_data, interval)\n",
    "# Visulize the data\n",
    "fig, ax = plt.subplots()\n",
    "x_plot = torch.linspace(-3., 3., 1000)\n",
    "y_plot = x_plot + 0.7 * torch.sin(3 * x_plot)\n",
    "ax.plot(x_train, y_train, '.')\n",
    "ax.plot(x_plot, y_plot, '-', color='red')\n",
    "\n",
    "ax.legend(('Simulated Datapoints','Simulator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "MNdkuXpIokwf",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0705f767a9e1c3d784990ac0d9028779",
     "grade": false,
     "grade_id": "cell-8f08589a03f117e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# template for problem 3\n",
    "# We define a multivariate Bayesian linear regression model, which has input_dim features and output_dim outputs\n",
    "class linear_regression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, sigma = 1.):\n",
    "        super(linear_regression, self).__init__()\n",
    "        \n",
    "        # Define the input and output dimension of the LR model\n",
    "        # In this example, input_dim and output_dim are both 1;\n",
    "        # They can be other integers when this class is used as the Bayesian neural network layers \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # set standard deviation of the prior (the $\\sigma_w$)\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        scale = 1. * np.sqrt(6. / (input_dim + output_dim))        \n",
    "        # EXERCISE: Initialize the approximated posterior distribution over the weight and bias terms\n",
    "        # (i.e. specify values for the corresponding variational parameters).\n",
    "        # All the weights are assumed independent from each other.\n",
    "        # Initialize the mean parameters from a uniform distribution over (-scale, scale) to improve stability.\n",
    "        # Instead of parametrizing the standard deviation sigma directly, we parametrize it using rho:\n",
    "        # sigma = log(1 + exp(rho)) to keep it positive during training.\n",
    "        # This way we don't need to use a positivity constraint during optimization.\n",
    "        \n",
    "        self.mu_bias = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-scale, scale)) # given as example\n",
    "        # self.rho_bias = ?\n",
    "        # self.mu_weights = ?\n",
    "        # self.rho_weights = ?\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, x, stochastic_flag):\n",
    "        eps = 1e-7 \n",
    "        \n",
    "        # Compute the standard deviation according to previous parametrization.\n",
    "        sigma_weights= torch.log(1 + torch.exp(self.rho_weights))\n",
    "        sigma_bias = torch.log(1 + torch.exp(self.rho_bias))\n",
    "        \n",
    "        if stochastic_flag:           \n",
    "            # stochastic forward pass during training\n",
    "            \n",
    "            # EXERCISE: Sample one set of weights from the current posterior approximation. \n",
    "            # These sampled weights will then be used to complete a forward pass for a mini-batch of data.\n",
    "            # Hints: you should first generate a sample from a standard normal \n",
    "            # distribution (epsilon-weights, epsilon-bias) and transform it to the\n",
    "            # posterior distribution (weights, bias) according to the posterior mean\n",
    "            # and variance (this is the 'reparametrization trick')\n",
    "            \n",
    "            epsilon_bias = torch.randn(self.output_dim) # shown as an example\n",
    "            # epsilon_weights = ?\n",
    "            # bias = ?\n",
    "            # weights = ? \n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "            # forward pass for a mini-batch\n",
    "            output = torch.mm(x, weights) + bias\n",
    "            \n",
    "        else:\n",
    "            # forward pass with the mean of posterior distribution during testing\n",
    "            output = torch.mm(x, self.mu_weights) + self.mu_bias\n",
    "\n",
    "        # calculate KL\n",
    "        # EXERCISE: calculate the KL divergence between the prior and the posterior        \n",
    "        # Hint: It is the solution you have computed in problem 1; the summation \n",
    "        # of the KL between two one dimensional Gaussian distributions\n",
    "        # KL_weights = ? \n",
    "        # KL_bias = ?\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        KL = KL_weights + KL_bias             \n",
    "        return output, KL   \n",
    "\n",
    "def training(blr, x, y, x_test, y_test, sigma_l, learning_rate = 0.001, batch_size = 10, num_epoch=100):\n",
    "    \n",
    "    # Set the parameters that you want to optimize during training\n",
    "    parameters = set(blr.parameters())\n",
    "    \n",
    "    # We use Adam to do optimization, with learning rate equals to learning_rate, eps is used to stablize the training\n",
    "    optimizer = optim.Adam(parameters, lr = learning_rate, eps=1e-3)\n",
    "    \n",
    "    # We use MSE loss since it's a regression problem\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "\n",
    "    num_data, num_dim = x.shape\n",
    "    y = y.view(-1, 1)\n",
    "    data = torch.cat((x, y), 1)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        # We permute the data for each epoch to decorrelate the training process\n",
    "        data_perm = data[torch.randperm(len(data))]\n",
    "        x = data_perm[:, 0:-1]\n",
    "        y = data_perm[:, -1]\n",
    "    \n",
    "        for index in range(int(num_data/batch_size)):\n",
    "            inputs = x[index*batch_size : (index+1)*batch_size]\n",
    "            labels = y[index*batch_size : (index+1)*batch_size].view(-1,1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward passing for one mini-batch of data, and calculate the KL\n",
    "            output, kl = blr(inputs, stochastic_flag=True)\n",
    "            \n",
    "            # Exercise: Calculate the value of the loss, the negative \n",
    "            # ELBO, from the outputs of the linear regression model (output, kl)\n",
    "            # Hint: the expected negative log-likelihood can be estimated by the MSE \n",
    "            # divided by (2*variance) for Gaussian likelihood functions (allowing \n",
    "            # you to use the 'criterion' defined above).\n",
    "            \n",
    "            # loss = ?\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "            # backpropogate the gradient     \n",
    "            loss.backward()\n",
    "            # optimize with SGD\n",
    "            optimizer.step()\n",
    "            \n",
    "        # calculate the training loss after one epoach \n",
    "        output_x, _= blr(x, stochastic_flag = False)\n",
    "        train_errors.append(criterion(output_x, y.view(-1,1)))\n",
    "        \n",
    "        # calculate the validation loss after one epoach \n",
    "        output_x_test, _ = blr(x_test, stochastic_flag = False)\n",
    "        val_errors.append(criterion(output_x_test, y_test.view(-1,1)))\n",
    "\n",
    "        if (epoch % 100) == 0:\n",
    "            print('EPOCH %d: TRAIN LOSS: %.4f; VAL LOSS IS: %.5f.'% (epoch+1, train_errors[epoch], val_errors[epoch]))        \n",
    "\n",
    "            \n",
    "# train the model \n",
    "num_input = 1; num_output = 1\n",
    "BLR = linear_regression(num_input, num_output)\n",
    "\n",
    "# Setting all the hyper-parameters\n",
    "learning_rate = 1e-2\n",
    "batch_size = 50; num_epoch = 500; sigma_l = .4\n",
    "training(BLR, x_train, y_train, x_val, y_val, sigma_l, learning_rate, batch_size, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suWSHyPuokwh"
   },
   "outputs": [],
   "source": [
    "## testing the trained BLR \n",
    "## NOTE: with this code you can test your above implementation. \n",
    "# We calculate the true values of x_plot\n",
    "x_plot = torch.linspace(-3., 3., 1000)\n",
    "y_plot = x_plot + 0.7 * torch.sin(3 * x_plot)\n",
    "\n",
    "# One benefit of being a Bayesian is that you can capture the predictive uncertainty: \n",
    "# Use the stochastic forward passing during prediction, and calculate the sample mean and \n",
    "# sample standard deviation of predictions for different sets of weights.\n",
    "\n",
    "iteration = 100;\n",
    "x_pred = []\n",
    "for i in range(iteration):\n",
    "    stochastic_flag = True\n",
    "    x_pred.append(BLR(x_plot.view(-1,1), stochastic_flag)[0].view(-1).tolist())\n",
    "x_pred = np.array(x_pred)\n",
    "\n",
    "# Calculate the mean and standard deviation of prediction according to the samples\n",
    "x_pred_mean = np.mean(x_pred, axis = 0)\n",
    "x_pred_std = np.std(x_pred, axis = 0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train, y_train, '.')\n",
    "ax.plot(x_plot, y_plot, '-', color='red')\n",
    "\n",
    "# Draw the mean of the prediction and also corresponding 95% crediable intervals.\n",
    "ax.plot(x_plot, x_pred_mean, '-', color = 'deepskyblue')\n",
    "ax.plot(x_plot, x_pred_mean - 2 * (x_pred_std+sigma_l), '-', color = 'skyblue')\n",
    "ax.plot(x_plot, x_pred_mean + 2 * (x_pred_std+sigma_l), '-', color = 'skyblue')\n",
    "\n",
    "ax.legend(('Simulated Datapoints','Simulator', 'Prediction Mean', '95% Prediction CI'))\n",
    "\n",
    "# We can see that Bayesian linear regression cannot fit the data perfectly, because the simulator \n",
    "# that generates the data is nonlinear. However, the 95% crediable interval covers the true target \n",
    "# nearly all the time (95%), which means we can still know the possible interval of the target \n",
    "# even the model is misspecified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0uuHNKJokwj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "exercise_09.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
