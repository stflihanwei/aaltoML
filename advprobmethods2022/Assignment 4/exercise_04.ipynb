{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ae2ec8e5d9cf0401da9b9ec612b3fbc",
     "grade": false,
     "grade_id": "cell-105d0154f52123fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2022)\n",
    "\n",
    "Pekka Marttinen, Prayag Tiwari, Vishnu Raj, Tianyu Cui, Yogesh Kumar, Antti Pöllänen, Louis Filstroff, Alex Aushev, Zheyang Shen, Nikitin Alexander , Sebastiaan De Peuter.\n",
    "\n",
    "## Exercise 4, due on Tuesday February 15th at 23:50.\n",
    "\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: ML-II for linear model\n",
    "2. Problem 2: Optimizing hyperparameters with validation set\n",
    "3. Problem 3: Poisson regression with Laplace approximation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c5b322c466f0c593604dbf5fc5d930c",
     "grade": false,
     "grade_id": "cell-108d1b689afb4668",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 1: ML-II for a linear model\n",
    "\n",
    "Fit the Bayesian linear parameter model to a given data 'ex4_1_data.txt' using the ML II approach. Optimize the hyperparameters $\\alpha$ and $\\beta$ using grid search. Complete the template given below with your own code. Make predictions for the test data using the fitted model and compute the mean squared error for test data. Also plot the data and the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code for problems 1 and 2. \n",
    "# NOTE: variables defined in this cell are used in code templates for problem 1 and 2.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# INITIALIZATION\n",
    "# Load the observations\n",
    "data = np.loadtxt('/coursedata/ex4_1_data.txt')\n",
    "x_obs = data[:,0]\n",
    "y_obs = data[:,1]\n",
    "\n",
    "# Training data\n",
    "N_train = 40\n",
    "x_train = x_obs[:N_train]\n",
    "y_train = y_obs[:N_train]\n",
    "\n",
    "# Validation data\n",
    "N_valid = 10\n",
    "x_valid = x_obs[N_train:N_train+N_valid]\n",
    "y_valid = y_obs[N_train:N_train+N_valid]\n",
    "\n",
    "# Testing data\n",
    "N_test = 10\n",
    "x_test = x_obs[N_train+N_valid:]\n",
    "y_test = y_obs[N_train+N_valid:]\n",
    "\n",
    "\n",
    "x_range = (-5, 5) # Possible values of x are in this range\n",
    "\n",
    "# Basis function parameters\n",
    "num_basis_functions = 11\n",
    "centers = np.linspace(x_range[0], x_range[1], num_basis_functions)\n",
    "lambdaval = 0.17\n",
    "# You can use here assume the correct basis function centers and lambda ...\n",
    "def rbf(x, centers, lambdaval):\n",
    "    # Radial Basis Function output for input x\n",
    "    #\n",
    "    # Inputs:\n",
    "    # x : input points (one-dimensional array)\n",
    "    # centers : basis function centers (one-dimensional array)\n",
    "    # lambdaval : basis function width (scalar)\n",
    "    #\n",
    "    # Output:\n",
    "    # Radial Basis Functions evaluated at x (two-dimensional array with len(x)\n",
    "    #                                        rows and len(centers) columns)\n",
    "    d = x[:,np.newaxis] - centers[np.newaxis,:]\n",
    "    y = np.exp(-0.5 * (d ** 2) / lambdaval)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "223ecb407436b1440361a7b3371a0aec",
     "grade": false,
     "grade_id": "cell-db646c7ed190e20f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Template for problem 1\n",
    "def bayesian_linear_regression(phi_x, y, alpha, beta):\n",
    "    # Bayesian linear parameter model\n",
    "    #\n",
    "    # Inputs:\n",
    "    # phi_x : the basis function applied to x-data (two-dimensional array)\n",
    "    # y : y-data (one-dimensional array)\n",
    "    # alpha : the precision of the weight prior distribution (scalar)\n",
    "    # beta : the precision of the assumed gaussian noise (scalar)\n",
    "    #\n",
    "    # Output:\n",
    "    # the posterior mean, the posterior covariance, the log marginal likelihood\n",
    "\n",
    "    N, B = phi_x.shape\n",
    "    \n",
    "    \n",
    "    # Add here code to compute:\n",
    "    # m = ? # EXERCISE: the posterior mean of w\n",
    "    # S = ? # EXERCISE: the posterior covariance of w\n",
    "    # S_inv = ? # EXERCISE: the inverse of S\n",
    "    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Note: This is a corrected version of equation 18.1.19 from Barbers book\n",
    "    d = beta * np.dot(phi_x.T, y)\n",
    "    log_likelihood = 0.5 * (-beta * np.dot(y, y) + d @ S @ d + np.log(np.linalg.det(2 * np.pi * S)) +\n",
    "                            B * np.log(alpha) + N * np.log(beta) - N * np.log(2 * np.pi))\n",
    "    return m, S, log_likelihood\n",
    "\n",
    "# Specify possible values for the alpha and beta parameters to test\n",
    "alphas = np.logspace(-3, 3, 100)\n",
    "betas = np.logspace(-3, 3, 100)\n",
    "\n",
    "# Grid search over possible values of alpha and beta\n",
    "best_log_likelihood = -np.inf # optimal parameter values maximize the log likelihood \n",
    "for alpha in alphas:\n",
    "    for beta in betas:\n",
    "        # Use here functions rbf and bayesian_linear_regression to compute the\n",
    "        # log marginal likelihood for given alpha and beta\n",
    "       \n",
    "        # What are the optimal values of alpha and beta, that maximize the marginal likelihood?\n",
    "        \n",
    "        # best_alpha = ? # EXERCISE\n",
    "        # best_beta = ? # EXERCISE\n",
    "\n",
    "        # Fit the model one more time using the optimal alpha and beta and the training data \n",
    "        # to get m for the optimal model\n",
    "        \n",
    "        # best_m = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# Compute the final regression function\n",
    "x_coord = np.linspace(x_range[0], x_range[1], 100)\n",
    "\n",
    "# Compute the predicted values for inputs in x_coord using best_m\n",
    "# y_mean = ? # EXERCISE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Plot the final learned regression function, together with the samples\n",
    "plt.plot(x_coord, y_mean, label=\"learned model\")\n",
    "plt.plot(x_train, y_train, 'kx', label=\"training data\")\n",
    "plt.plot(x_test, y_test, 'rx', label=\"testing data\")\n",
    "\n",
    "# Make predictions for inputs in the test data, so that you get\n",
    "# predictions 'y_pred' for inputs in x_test.\n",
    "# y_pred = ? #EXERCISE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# print(y_pred)\n",
    "\n",
    "# Plot the predictions\n",
    "plt.plot(x_test, y_pred, 'gx', label=\"testing predictions\")\n",
    "\n",
    "# Compute the mean squared prediction error for the test data.\n",
    "# mse_test = ???\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"ML-II: $\\\\alpha$=%.3f, $\\\\beta$=%.3f, mse=%.4f\" %\n",
    "          (best_alpha, best_beta, mse_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fa4d100c8298f684723ddf9da4cdef9",
     "grade": false,
     "grade_id": "cell-d66f4787cf0dc30c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: Optimizing hyperparameters with validation set\n",
    "\n",
    "As in problem 1, fit the Bayesian linear parameter model to a given data 'ex4_1_data.txt', but optimize the hyperparameters $\\alpha$ and $\\beta$ by dividing the training data into training and validation sets, and selecting the values of $\\alpha$  and $\\beta$ that minimize the mean squared error for the validation set. Make predictions for the test data using the fitted model and compute the mean squared error for test data. Plot the data and the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60e3a1a0b13627ee213e3f5cd4d7c889",
     "grade": false,
     "grade_id": "cell-e3d4dd1084e72af3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Template for problem 2\n",
    "# Specify possible values for the alpha and beta parameters to test\n",
    "alphas = np.logspace(-3, 3, 100)\n",
    "betas = np.logspace(-3, 3, 100)\n",
    "\n",
    "# Grid search over possible values of alpha and beta\n",
    "mse_valid = np.zeros((len(alphas), len(betas)))\n",
    "for a, alpha in enumerate(alphas):\n",
    "    for b, beta in enumerate(betas):\n",
    "        \n",
    "        # Use here functions rbf and bayesian_linear_regression to fit the\n",
    "        # model and compute the prediction error (using mean squared error)\n",
    "        # for the validation data\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# What are the optimal values of alpha and beta, that minimize the prediction error in the validation data?\n",
    "# best_alpha = ? #EXERCISE\n",
    "# best_beta = ?  #EXERCISE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Fit the model one more time using the optimal alpha and beta and all data \n",
    "# available for model fitting (both training and validation sets)\n",
    "# best_m = ? #EXERCISE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "x_coord = np.linspace(x_range[0], x_range[1], 100)\n",
    "\n",
    "# Compute the predicted values for inputs in x_coord using best_m\n",
    "# y_mean = ? #EXERCISE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Plot the final learned regression function, together with the samples\n",
    "plt.plot(x_coord, y_mean, label=\"learned model\")\n",
    "plt.plot(x_train, y_train, 'kx', label=\"training data\")\n",
    "plt.plot(x_valid, y_valid, 'bx', label=\"validation data\")\n",
    "plt.plot(x_test, y_test, 'rx', label=\"testing data\")\n",
    "\n",
    "# Make predictions for inputs in the test data, so that you get\n",
    "# predictions 'y_pred' for inputs in x_test.\n",
    "# y_pred = ? ? # EXERCISE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Plot the predictions\n",
    "plt.plot(x_test, y_pred, 'gx', label=\"testing predictions\")\n",
    "\n",
    "# Compute the mean squared prediction error for the test data.\n",
    "# mse_test = ? # EXERCISE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Validation: $\\\\alpha$=%.3f, $\\\\beta$=%.3f, mse=%.4f\" %\n",
    "          (best_alpha, best_beta, mse_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3d429c120619a89d90fa6cd8349857a",
     "grade": false,
     "grade_id": "cell-ba07b716ed50268e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 3: Poisson regression with Laplace approximation\n",
    "\n",
    "Poisson regression can be used to model count data. A Poisson regression model can be defined as\n",
    "\\begin{align}\n",
    "    y_i \\mid \\boldsymbol{\\theta} & \\sim \\operatorname{Poisson}(\\exp(\\boldsymbol{\\theta}^T \\mathbf{x}_i )) \\\\\n",
    "    \\theta &\\sim \\mathcal{N}(\\mathbf{0}, \\alpha^{-1} \\mathbf{I})\n",
    "\\end{align}\n",
    "where $y_i$ are the observed counts, $\\mathbf{x}_i$ the related covariates, $i = 1,\\ldots,N$, and $\\boldsymbol{\\theta}^T$ are the regression weights. In this exercise, we approximate the posterior $p(\\boldsymbol{\\theta} \\mid \\mathbf{y})$ using the Laplace approximation. We will do this in two steps. In the first step we will derive the gradient $-\\nabla \\log p(\\boldsymbol{\\theta} \\mid \\mathbf{y} )$ and in the second step we will write Laplace approximation. And finally, we will look compare the true density with the laplace approximation.  \n",
    "\n",
    "**(a)** Derive the gradient $-\\nabla \\log p(\\boldsymbol{\\theta} \\mid \\mathbf{y} )$ and the Hessian $\\mathbf{H} = -\\nabla\\nabla\\log p( \\boldsymbol{\\theta} \\mid y)$ needed for the Laplace approximation. \n",
    "\n",
    "**(b)** Write the Laplace approximation as the density of a Gaussian distribution. What is the mean and the covariance matrix of this distribution?  \n",
    "\n",
    "**(c)** Compare the Laplace approximation to the true posterior (computed using numerical integration), in a case where we have one-dimensional covariates only. Use data given in the file 'ex4\\_4\\_data.txt' and hyperparameter value $\\alpha = 10^{-2}$. Plot the two posteriors and the true value $\\theta = \\pi/4$ used to generate the data. Also plot the data with the regression line $\\hat y_i = \\exp(\\hat\\theta x_i)$ using the MAP estimate $\\hat{\\boldsymbol{\\theta}}$. The code template below at the end of the notebook will help with this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions to 3a and 3b\n",
    "Write your solutions to __(3a)__ and __(3b)__ in LateX or attach a picture in this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ff630a002317aa0fb097d07bffbcb27",
     "grade": false,
     "grade_id": "cell-616aa2bfeb874387",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Template for problem 3(c)\n",
    "# NOTE: starter code at the begining of the notebook and function 'bayesian_linear_regression' \n",
    "#       is required to run this cell\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import factorial\n",
    "from scipy.stats import norm\n",
    "\n",
    "# get some data\n",
    "data = np.loadtxt('/coursedata/ex4_4_data.txt')\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "\n",
    "theta_true = np.pi / 4 # true parameter used to generate the data\n",
    "alpha = 1e-2 # prior's parameter\n",
    "\n",
    "# compute Laplace approximation\n",
    "theta_lapl = 0.5 # initial\n",
    "# iterate to optimum with newton's method to find the MAP estimate for theta\n",
    "for iter in range(100):\n",
    "    # compute gradient\n",
    "    # grad = ???\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # compute Hessian\n",
    "    # H = ???\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    theta_lapl = theta_lapl - grad / H # do newton step\n",
    "\n",
    "# compute Hessian at optimum\n",
    "# H = ???\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(H)\n",
    "difference = theta_lapl - theta_true\n",
    "\n",
    "# plot posterior densities\n",
    "theta = np.linspace(0.55, 0.95, 1000)\n",
    "post_true = np.zeros(len(theta))\n",
    "for i in range(len(theta)):\n",
    "    # log posterior:    \n",
    "    post_true[i] = (np.dot(y, x * theta[i]) - np.sum(np.exp(x * theta[i]) -\n",
    "                    np.log(factorial(y))) - 0.5*alpha*np.dot(theta[i], theta[i]))\n",
    "\n",
    "M = np.amax(post_true)\n",
    "post_true = np.exp(post_true-M) / np.sum(np.exp(post_true-M)) / (theta[1]-theta[0]) # normalize\n",
    "\n",
    "# compute approximative density at the points 'theta'\n",
    "# Hint: you can use norm.pdf from scipy.stats\n",
    "# post_laplace = ???\n",
    "               \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "    \n",
    "plt.figure(1)\n",
    "plt.plot(theta, post_true, '-k', label=\"True posterior\")\n",
    "plt.plot(theta, post_laplace, '-.r', label=\"Laplace approximation\")\n",
    "plt.plot(theta_true, 0, 'o', label=\"True value\")\n",
    "plt.xlim(0.55, 0.95)\n",
    "plt.xlabel('$\\\\theta$')\n",
    "plt.title('Posterior $p(\\\\theta|y)$')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(x, y, 'o', x, np.exp(theta_lapl*x), '-r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
