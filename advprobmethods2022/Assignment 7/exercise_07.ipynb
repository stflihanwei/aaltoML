{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60477290a164fe532274c7200c552103",
     "grade": false,
     "grade_id": "cell-1ed8add4ed57ade6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2022)\n",
    "\n",
    "Pekka Marttinen, Prayag Tiwari, Vishnu Raj, Tianyu Cui, Yogesh Kumar, Antti Pöllänen, Louis Filstroff, Alex Aushev, Zheyang Shen, Nikitin Alexander , Sebastiaan De Peuter.\n",
    "\n",
    "## Exercise 7, due on Tuesday March 22 at 23:50.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: ELBO for simple model (1/2)\n",
    "2. Problem 2: ELBO for simple model (2/2)\n",
    "3. Problem 3: Bayes factors\n",
    "4. Problem 4: Model selection for GMM with BIC and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e428aa463e9ab9e9607592c770500cd1",
     "grade": false,
     "grade_id": "cell-b7c9831bb5e281bd",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 1: ELBO for the simple model (1/2)\n",
    "This problem and the next deal with deriving the ELBO for the ‘simple model’, described in the PDF document 'simple_elbo.pdf'. Before doing these exercises, familiarize yourself with the contents of the document.\n",
    "\n",
    "**(a)** Show that the general formula for ELBO, shown in Equation (8), can be written as the sum shown in Equation (9) for the simple model.\n",
    "\n",
    "**(b)** Derive the 2nd term $E_{q(\\theta)}[log p(\\theta)]$ of the ELBO. (**Hint:** recall that $Var(X)=E(X^2)-E(X)^2$).\n",
    "\n",
    "**(c)** Find out the formula for the 7th term $E_{q(\\theta)[log q(\\theta)]}$ of the ELBO. (**Hint:** see the 6th term)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 1 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f1df62656e3d17cbfbefd0aa87882ff",
     "grade": false,
     "grade_id": "cell-3b2d5183dc701b48",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: ELBO for the simple model (2/2)\n",
    "\n",
    "**(a)** Derive the 4th term $ E_{q(z)q(\\theta)}\\left[\\log p(x \\mid z, \\theta)\\right]$ of the ELBO. \n",
    "\n",
    "**Hint 1**: $E(XY) = E(X)E(Y)$ if X and Y are assumed indendent (see how this is already used in the derivation of term 3). \n",
    "\n",
    "**Hint 2**: $E(X-Y) = E(X - a + a - Y)^2$.\n",
    "\n",
    "**(b)** Implement terms 2, 4, and 7 in the code template given below. Verify that the ELBO increases when you run the VB algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 2(a) here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca18c649f4a45961c14e2d47877c4504",
     "grade": false,
     "grade_id": "cell-9577326940b26f48",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Template for problem 2(b). \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(123123123)\n",
    "\n",
    "# Compute ELBO for the model described in simple_elbo.pdf\n",
    "def compute_elbo(alpha_tau, beta_tau, r1, r2, m2, beta2, alpha0, beta0, x):\n",
    "    \n",
    "    from scipy.special import psi, gammaln # digamma function, logarithm of gamma function\n",
    "    \n",
    "    # E[log p(tau)]\n",
    "    term1 = (alpha0 - 1) * (psi(alpha_tau) + psi(beta_tau) - 2 * psi(alpha_tau + beta_tau))\n",
    "\n",
    "    # E[log p(theta)]\n",
    "    # term2 = ?\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # E[log p(z|tau)]\n",
    "    N2 = np.sum(r2); N1 = np.sum(r1); N = N1 + N2\n",
    "    term3 = N2 * psi(alpha_tau) + N1 * psi(beta_tau) - N * psi(alpha_tau + beta_tau)\n",
    "\n",
    "    # E[log p(x|z,theta)]\n",
    "    # term4 = ?\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "\n",
    "    # Negative entropy of q(z)\n",
    "    term5 = np.sum(r1 * np.log(r1)) + np.sum(r2 * np.log(r2))\n",
    "\n",
    "    # Negative entropy of q(tau)\n",
    "    term6 = (gammaln(alpha_tau + beta_tau) - gammaln(alpha_tau) - gammaln(beta_tau)\n",
    "        + (alpha_tau - 1) * psi(alpha_tau) + (beta_tau - 1) * psi(beta_tau)\n",
    "        - (alpha_tau + beta_tau - 2) * psi(alpha_tau + beta_tau))\n",
    "\n",
    "    # Negative entropy of q(theta)\n",
    "    # term7 = ?\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    elbo = term1 + term2 + term3 + term4 - term5 - term6 - term7\n",
    "    \n",
    "    return elbo\n",
    "\n",
    "\n",
    "# Simulate data\n",
    "theta_true = 4\n",
    "tau_true = 0.3\n",
    "n_samples = 10000\n",
    "z = (np.random.rand(n_samples) < tau_true)  # True with probability tau_true\n",
    "x = np.random.randn(n_samples) + z * theta_true\n",
    "\n",
    "# Parameters of the prior distributions.\n",
    "alpha0 = 0.5\n",
    "beta0 = 0.2\n",
    "\n",
    "n_iter = 15 # The number of iterations\n",
    "elbo_array = np.zeros(n_iter) # To track the elbo\n",
    "\n",
    "# Some initial value for the things that will be updated\n",
    "E_log_tau = -0.7   # E(log(tau))\n",
    "E_log_tau_c = -0.7  # E(log(1-tau))\n",
    "E_log_var = 4 * np.ones(n_samples)  # E((x_n-theta)^2)\n",
    "r2 = 0.5 * np.ones(n_samples)  # Responsibilities of the second cluster.\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    # Updated of responsibilites, factor q(z)\n",
    "    log_rho1 = E_log_tau_c - 0.5 * np.log(2 * np.pi) - 0.5 * (x ** 2)\n",
    "    log_rho2 = E_log_tau - 0.5 * np.log(2 * np.pi) - 0.5 * E_log_var\n",
    "    max_log_rho = np.maximum(log_rho1, log_rho2)  # Normalize to avoid numerical problems when exponentiating.\n",
    "    rho1 = np.exp(log_rho1 - max_log_rho)\n",
    "    rho2 = np.exp(log_rho2 - max_log_rho)\n",
    "    r2 = rho2 / (rho1 + rho2)\n",
    "    r1 = 1 - r2\n",
    "    \n",
    "    N1 = np.sum(r1)\n",
    "    N2 = np.sum(r2)\n",
    "    \n",
    "    # Update of factor q(tau)\n",
    "    from scipy.special import psi # digamma function\n",
    "    E_log_tau = psi(N2 + alpha0) - psi(N1 + N2 + 2*alpha0)\n",
    "    E_log_tau_c = psi(N1 + alpha0) - psi(N1 + N2 + 2*alpha0)\n",
    "    \n",
    "    # Update of factor q(theta)\n",
    "    x2_avg = 1 / N2 * np.sum(r2 * x)\n",
    "    beta_2 = beta0 + N2\n",
    "    m2 = 1 / beta_2 * N2 * x2_avg\n",
    "    E_log_var = (x - m2) ** 2 + 1 / beta_2\n",
    "    \n",
    "    # Keep track of the current estimates\n",
    "    tau_est = (N2 + alpha0) / (N1 + N2 + 2*alpha0)\n",
    "    theta_est = m2\n",
    "    \n",
    "    # Compute ELBO\n",
    "    alpha_tau = N2 + alpha0\n",
    "    beta_tau = N1 + alpha0\n",
    "    elbo_array[i] = compute_elbo(alpha_tau, beta_tau, r1, r2, m2, beta_2, alpha0, beta0, x)\n",
    "\n",
    "# Plot ELBO as a function of iteration\n",
    "plt.plot(np.arange(n_iter) + 1, elbo_array)\n",
    "plt.xticks(np.arange(n_iter) + 1)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.title(\"ELBO\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13c14e7d13b2c2b31e1ff3e966683989",
     "grade": false,
     "grade_id": "cell-47f1618ab4fa1c14",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 3: Bayes factors\n",
    "Suppose we have two bags, each containing a large number of black and white marbles. To learn about the contents of the bags, we have done 5 draws from each bag. After each draw, the marble drawn has been returned to the bag. The draws from the first bag are as follows ( B, W, W, B, B ) and the draws from the second bag are ( B, B, B, B, W ) , where B corresponds to a Black marble and W to a White marble. \n",
    "\n",
    "Consider two models:\n",
    "\n",
    "- $M_1$ : the proportions of marbles are the same in the two bags\n",
    "\n",
    "- $M_2$ : the proportions of marbles are different in the two bags.\n",
    "\n",
    "**(a)** Write out the two models explicitly. Assuming that a priori all proportions are equally probable, compute the Bayes factor in favor of $M_1$.\n",
    "\n",
    "**(b)** The same as (a), but now the first set of draws contains 300 black and 200 white draws,\n",
    "and second set of draws 250 black and 250 white draws.\n",
    "\n",
    "**Hint**: Beta distribution is the conjugate prior for the Binomial/Bernoulli likelihood, and a uniform proportion corresponds to the $Beta( 1, 1)$ distribution.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 3 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68d31bb0c21986261cfa0587f77cd0ee",
     "grade": false,
     "grade_id": "cell-675a401a5f2fa552",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 4: Model selection for GMM with BIC and cross validation\n",
    "\n",
    "In many machine learning applications model selection is crucial. In this exercise, you\n",
    "will practice two common approaches for model selection: \n",
    "\n",
    "- Bayesian Information Criterion (BIC) (as an approximation to ‘Bayesian model selection’) and  \n",
    "\n",
    "- Cross-Validation (as a representative for a predictive model selection criterion).\n",
    "\n",
    "You are given a data set (1000 samples of dimension 2) contained in the file data.pickle, which has been sampled from a Gaussian Mixture Model (GMM) using three classes(the true class labels are given for your convenience, but they should not be used in learning the model).\n",
    "\n",
    "In the given code template below, the data will be divided into training and test sets.\n",
    "\n",
    "**(a)** Complete the functions 'compute_bic' and 'cross_validate'. Use both criteria to select the number of components in the GMM using the training data. Plot the both the BIC and the validation log-likelihoods as a function of the number of components, as well as the data with the best model. Do both methods find a model with three components as the most likely?\n",
    "\n",
    "**(b)** Use the selected models to evaluate the test set log-likelihood.\n",
    "\n",
    "**(c)** Explain briefly the pros and cons of the two approaches and comment which approach you would consider better and why.\n",
    "\n",
    "**Hint**: What is the total number of parameters needed to specify the component\n",
    "means and covariance matrices, and the mixture weights? You will need this\n",
    "number to compute BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Starter code for problem 4. The solution template is in the cell below.\n",
    "# Tools for learning Gaussian mixture models; adapted from the BRMLtoolkit by David Barber.\n",
    "import numpy as np\n",
    "import numpy.matlib as matlib\n",
    "import numpy.linalg as LA\n",
    "#import scipy.misc\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "def condp(X):\n",
    "    return X / np.sum(X, axis=0)\n",
    "\n",
    "def condexp(logp):\n",
    "    pmax = np.max(logp, axis=0)\n",
    "    P = logp.shape[0]\n",
    "    return condp(np.exp(logp - np.tile(pmax, (P, 1))))\n",
    "\n",
    "def GMMlogp(X, H, P, m, S):\n",
    "    D, N = X.shape  # dimension and number of points\n",
    "\n",
    "    logp = np.zeros((N, H))\n",
    "    for i in range(H):\n",
    "        invSi = LA.inv(S[i,:,:])\n",
    "        sign, logdetSi = LA.slogdet(2 * np.pi * S[i,:,:])\n",
    "\n",
    "        for n in range(N):\n",
    "            v = X[:,n] - m[:,i]\n",
    "            logp[n,i] = -0.5 * (v @ invSi @ v) - 0.5 * logdetSi + np.log(P[i])\n",
    "\n",
    "    return logp\n",
    "\n",
    "\n",
    "\n",
    "# Log Likelihood of data X under a Gaussian Mixture Model\n",
    "#\n",
    "# X : each column of X is a datapoint.\n",
    "# P : mixture coefficients\n",
    "# m : means\n",
    "# S : covariances\n",
    "#\n",
    "# Returns: A list containing the log likelihood for each data point in X\n",
    "def GMMloglik(X, P, m, S):\n",
    "    N = X.shape[1]\n",
    "    H = m.shape[1]\n",
    "\n",
    "    logp = GMMlogp(X, H, P, m, S)\n",
    "    logl = [logsumexp(a=logp[n,:], b=np.ones(H)) for n in range(N)]\n",
    "\n",
    "    return logl\n",
    "\n",
    "# Fit a mixture of Gaussian to the data X using EM\n",
    "#\n",
    "# X : each column of X is a datapoint.\n",
    "# H : number of components of the mixture.\n",
    "# n_iter : number of EM iterations\n",
    "#\n",
    "# Returns: (P, m, S, loglik, phgn)\n",
    "# P : learned mixture coefficients\n",
    "# m : learned means\n",
    "# S : learned covariances\n",
    "# loglik : log likelihood of the learned model\n",
    "# phgn : mixture assignment probabilities\n",
    "def GMMem(X, H, n_iter):\n",
    "    D, N = X.shape  # dimension and number of points\n",
    "\n",
    "    # initialise the centres to random datapoints\n",
    "    r = np.random.permutation(N)\n",
    "    m = X[:, r[:H]]\n",
    "\n",
    "    # initialise the variances to be large\n",
    "    s2 = np.mean(np.diag(np.cov(X)))\n",
    "    S = matlib.tile(s2 * np.eye(D), [H, 1, 1])\n",
    "\n",
    "    # intialise the component probilities to be uniform\n",
    "    P = np.ones(H) / H\n",
    "\n",
    "    for emloop in range(n_iter):\n",
    "        # E-step:\n",
    "        logpold = GMMlogp(X, H, P, m, S)\n",
    "\n",
    "        phgn = condexp(logpold.T)  # responsibilities\n",
    "        pngh = condp(phgn.T)       # membership\n",
    "\n",
    "        # M-step:\n",
    "        for i in range(H):   # now get the new parameters for each component\n",
    "            tmp = (X - np.tile(m[:,i:i+1], N)) * np.tile(np.sqrt(pngh[:,i]), (D,1))\n",
    "            Scand = np.dot(tmp, tmp.T)\n",
    "\n",
    "            if LA.det(Scand) > 0.0001:   # don't accept too low determinant\n",
    "                S[i,:,:] = Scand\n",
    "\n",
    "        m = np.dot(X, pngh)\n",
    "        P = np.sum(phgn, axis=1) / N\n",
    "\n",
    "    logl = np.sum(logsumexp(a=logpold[n,:], b=np.ones(H)) for n in range(N))\n",
    "\n",
    "    return P, m, S, logl, phgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1349a1524033d0a1cc56845a976af105",
     "grade": false,
     "grade_id": "cell-339a70e9272824f5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Template for problem 4\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "np.random.seed(0)\n",
    "totalComponents = 5  # max number of mixture components\n",
    "criterion_flag = 0    # 0: cross validation, 1: BIC\n",
    "\n",
    "def compute_bic(Xtrain):\n",
    "    BICs = []\n",
    "    for H in range(1, totalComponents+1):    # number of mixture components\n",
    "        print(\"H: {}\".format(H))\n",
    "\n",
    "        P, m, S, loglik, phgn = GMMem(Xtrain, H, 100)  # fit to data\n",
    "\n",
    "        # numParams = ?     # number of parameters in the model\n",
    "        # BIC = ?           # BIC for the model\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        BICs.append(BIC)\n",
    "    return BICs\n",
    "\n",
    "def cross_validate(Xtrain):\n",
    "    foldCount = 5    # number of folds\n",
    "\n",
    "    loglik = np.zeros((totalComponents, foldCount))\n",
    "\n",
    "    Nlearning = Xtrain.shape[1]\n",
    "    order = np.random.permutation(Nlearning)    # to randomize the sample order\n",
    "\n",
    "    for H in range(1, totalComponents+1):     # number of mixture components\n",
    "        print(\"H: {}\".format(H))\n",
    "\n",
    "        for fold in range(foldCount):    # K-fold cross validation (K=5)\n",
    "            ind = fold * int(Nlearning/foldCount) + np.arange(int(Nlearning/foldCount))\n",
    "            val_indices = order[ind]\n",
    "\n",
    "            training_indices = np.setdiff1d(np.arange(Nlearning), val_indices);\n",
    "\n",
    "            X_train = Xtrain[:,training_indices]  # cv training data\n",
    "            X_val   = Xtrain[:,val_indices]       # cv validation data\n",
    "\n",
    "            # train model\n",
    "            P1, m1, S1, loglik1, phgn1 = GMMem(X_train, H, 100)   # fit model\n",
    "\n",
    "            # Predict using the cv trained model\n",
    "            # logl1 = ?\n",
    "            # loglik[H-1,fold] = ?\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "    return loglik\n",
    "    \n",
    "\n",
    "# load data\n",
    "with open(\"/coursedata/data.pickle\", \"rb\") as f:\n",
    "    X, labels = pickle.load(f)\n",
    "\n",
    "D, N = X.shape   # dimension and number of data points\n",
    "\n",
    "ratio = 0.75\n",
    "train_ind = np.random.choice(N, int(ratio * N), replace=False)   # training data index\n",
    "test_ind = np.setdiff1d(np.arange(N), train_ind)                 # test data index\n",
    "\n",
    "Xtrain = X[:,train_ind]            # training data\n",
    "Xtrain_labels = labels[train_ind]  # training data labels\n",
    "\n",
    "Xtest = X[:,test_ind]            # test data\n",
    "Xtest_labels = labels[test_ind]  # test data labels\n",
    "\n",
    "# plot training and test data\n",
    "def plot_data():\n",
    "    for i in sorted(set(Xtrain_labels)):\n",
    "        X_comp = Xtrain[:, Xtrain_labels == i]\n",
    "        plt.plot(X_comp[0], X_comp[1], '.' + 'brgmcyk'[i-1], markersize=6)\n",
    "\n",
    "    plt.plot(Xtest[0], Xtest[1], 'kd', markersize=4, markeredgewidth=0.5, markerfacecolor=\"None\")\n",
    "\n",
    "plot_data()\n",
    "plt.title('training data, test data (in black)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ***** Use BIC to select the number of components\n",
    "# (Only this part differs from the second template, where cross validation is used instead)\n",
    "\n",
    "if criterion_flag:\n",
    "    print('computing BIC')\n",
    "    scores = compute_bic(Xtrain)\n",
    "    ylabel = \"(BIC)\"\n",
    "elif not criterion_flag:\n",
    "    print('computing cross validation score')\n",
    "    scores = cross_validate(Xtrain)\n",
    "    ylabel = \"(Cross Validation)\"\n",
    "\n",
    "# plot the BIC curve\n",
    "plt.bar(np.arange(1, totalComponents+1), np.mean(scores, axis=1))\n",
    "#plt.yscale(\"log\", nonposy=\"clip\")\n",
    "plt.xlabel('Number of Mixture Components')\n",
    "plt.ylabel(ylabel)\n",
    "plt.title('Model Selection' + ylabel )\n",
    "plt.show()\n",
    "\n",
    "# select the number of mixture components which minimizes the BIC\n",
    "h = np.argmax(np.mean(scores, axis=1)) + 1\n",
    "\n",
    "\n",
    "\n",
    "# ***** TRAIN\n",
    "\n",
    "# Now train full model with selected number of mixture components\n",
    "P, m, S, loglik, phgn = GMMem(Xtrain, h, 100)  # fit to data\n",
    "\n",
    "# Predict using the full trained model (Use GMMem.GMMloglik)\n",
    "# logl = ?\n",
    "# print('Test Data Likelihood = {0:f}'.format(?))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Plot the best GMM model\n",
    "plot_data()\n",
    "\n",
    "for i in range(h):\n",
    "    dV, E = LA.eig(S[i,:,:])\n",
    "\n",
    "    theta = np.arange(0, 2*np.pi, 0.1)\n",
    "    p = np.sqrt(dV.reshape(D,1)) * [np.cos(theta), np.sin(theta)]\n",
    "    x = (E @ p) + np.tile(m[:,i:i+1], (1, len(theta)))\n",
    "\n",
    "    plt.plot(x[0], x[1], 'r-', linewidth=2)\n",
    "\n",
    "plt.title('training data, test data (in black)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
