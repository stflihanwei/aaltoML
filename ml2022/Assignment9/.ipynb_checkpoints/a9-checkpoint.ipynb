{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d1d81d263bf3e24d2dec3c011ac4ebb",
     "grade": false,
     "grade_id": "cell-39af320a33dce0a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 9 - Feature learning & selection\n",
    "This week, we'll go over some ways in which one may lower the amount of features used for training even without domain expertise, i.e., without being able to select them manually based on one's understanding of the relationships between the features and the label. There are multiple reasons one might want to use less features in training: \n",
    "* to prevent overfitting (improving the ratio of training samples to features)\n",
    "* to decrease computational complexity (faster training and prediction)\n",
    "* visualizing high-dimensional data\n",
    "\n",
    "In particular, we'll start off by looking at a method used even outside of machine learning called Principal component analysis (PCA) and use it to analyse and train on the MNIST dataset. Then we'll take a look at the Lasso and compare them. ### TODO: Possibly, At the end, we show parts of sklearn.feature_selection ###\n",
    "\n",
    "## Learning goals\n",
    "After this assignment, you should\n",
    "* understand why one might want to use a lower number of features\n",
    "* understand PCA on an intuitive level\n",
    "* be aware of the pitfalls of using PCA in machine learning\n",
    "* know what is Lasso and how it relates to Ridge regression\n",
    "* be able to use Lasso to select a subset of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2824e4ac020958c8c35f18517e47fb64",
     "grade": false,
     "grade_id": "cell-09dae34f317a81a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "You can think of Principal component analysis as finding a linear subspace of a given dimension (line, plane, hyperplane,...) that is the best fit of high-dimensional vectors. In this case, by best fit we mean that we minimize the squares of the distances of the vectors from the subspace, which corresponds to the information that we'll lose by this method.\n",
    "\n",
    "If you recall how linear regression works, this may sound familiar. The difference is that linear regression studies functional dependence of the label on the features (how to fit a linear predictor), while PCA does not take the labels into account, it is only concerned about the features. The exact formulations are out of scope of this assignment but for visualizations, see for example [this link](https://www.r-bloggers.com/2010/09/principal-component-analysis-pca-vs-ordinary-least-squares-ols-a-visual-explanation/). \n",
    "\n",
    "There are a few pitfalls to PCA that you should be aware of:\n",
    "* If we set the dimensions of the subspace too low, we may lose too much information to make any reasonable predictions.\n",
    "* The features generated by PCA are a linear combination of the original features and there's likely no way to interpret them. Imagine a dataset where you try to predict car brand based on engine power and price. Using PCA to reduce the features to 1 might leave us with a feature \"0.9\\*price-0.1\\*power\", which doesn't have any real-world meaning. \n",
    "* Since PCA doesn't take labels into account, it is entirely possible that the selected features will not be optimal for predicting the labels.\n",
    "* PCA is very sensitive to statistical properties of the different dimensions (=features).\n",
    "\n",
    "All of these will be illustrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d53b1276153829e8f08e0bca5ac3503",
     "grade": false,
     "grade_id": "cell-4c4121cba8900e98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### MNIST\n",
    "We begin by fetching the MNIST dataset. This dataset contains 70000 B&W images with resolution 28x28 pixels. Each pixel represents a shade of grey as an 8bit integer (range 0-255) and each image represents a handwritten digit. This leads to a 10-class classification problem, where the goal is to predict the digit based on pixel intensities.\n",
    "\n",
    "Below we fetch the datset and view one representative of each class to familiarize ourselves with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47dc041425633e67ea22675178890e05",
     "grade": false,
     "grade_id": "cell-218cc13d63a64b6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import basic libraries needed for this assignment\n",
    "%config Completer.use_jedi = False  # enable code auto-completion\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# load the MNIST dataset\n",
    "X_MNIST, y_MNIST = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e78ac4352c2a80bf0ffeaa3b0ebde89",
     "grade": false,
     "grade_id": "cell-f23d40be5b2a0017",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the numbers in two rows\n",
    "def plot_digits(X,y,cols=5,title=None):\n",
    "    cols = 5\n",
    "    fig, axs = plt.subplots(2, cols)\n",
    "    plt.axis('off')\n",
    "    for digit in range(10):\n",
    "        # find the first representative of the label\n",
    "        idx = np.argwhere(y == str(digit))[0]\n",
    "\n",
    "        # change the vector into a 2D array and plot it\n",
    "        im = X[idx].reshape(28, 28)\n",
    "        axs[digit//cols, digit % cols].set_title(f'Class {digit}')\n",
    "        axs[digit//cols, digit % cols].imshow(im, cmap='gray')\n",
    "        axs[digit//cols, digit % cols].axis('off')\n",
    "\n",
    "    # change the spacing between the subplots\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=-0.3)\n",
    "    if title is not None:\n",
    "        fig.suptitle(title,fontsize=14)\n",
    "        fig.subplots_adjust(top=0.97)\n",
    "    \n",
    "plot_digits(X_MNIST,y_MNIST,title=\"Examples of each class in the MNIST dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c6fc7265f73d839fe360efcbbee70b6",
     "grade": false,
     "grade_id": "cell-1a6338f0fb9c6c8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Demo: PCA, before and after\n",
    "Below, we use Logistic Regression like we're used to, to predict the digits. Even though the images have a rather low resolution, training the model on more than a couple hundred images quickly becomes very computationally demanding and can take a couple of minutes. Performing a grid search to find the optimal parameters could thus easily take over an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fde1c2e29b291435ebc056021c7eea5",
     "grade": false,
     "grade_id": "cell-7c7aa7d3595a7911",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_MNIST, y_MNIST, test_size=0.33, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear' )\n",
    "clf.fit(X_train[:200], y_train[:200])\n",
    "# can increase training data to reach 91.7 % but it takes 10 minutes on my computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47ab0cbff480e3cb4c1661ac1dcc9315",
     "grade": false,
     "grade_id": "cell-17e00e2aa1fdeee6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(y_true, y_pred):\n",
    "    # visualize the confusion matrix\n",
    "    ax = plt.subplot()\n",
    "    c_mat = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(c_mat, annot=True, fmt='g', ax=ax)\n",
    "\n",
    "    ax.set_xlabel('Predicted labels', fontsize=15)\n",
    "    ax.set_ylabel('True labels', fontsize=15)\n",
    "    ax.set_title('Confusion Matrix', fontsize=15)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "generate_confusion_matrix(y_val, y_pred)\n",
    "plt.show()\n",
    "\n",
    "# compute the accuracy\n",
    "multi_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Prediction accuracy: {100*multi_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "546c48857c24609e18c7289227e6b59b",
     "grade": false,
     "grade_id": "cell-1bb92b579b911ee9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### With PCA\n",
    "First, we fit the PCA to the training data. By setting the ``n_components`` parameter, we can choose the dimension of the fitted subspace (=number of output features). A perhaps surprising result is that having computed the PCA, it is very simple to lower the amount of features further: the most important feature (in the PCA sense) is the first one, the most important two are the first two,... \n",
    "\n",
    "One way to decide how many components to use is to look at the explained variance ratio of each of the features. These correspond to the \"importance\" of each feature. Let's plot these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abe4396e5fc795e805ea67c2ccfa5d18",
     "grade": false,
     "grade_id": "cell-9915473b3a665d02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# fit the PCA\n",
    "N = 50\n",
    "pca = PCA(n_components=N)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# plot the explained variances\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "color = 'tab:blue'\n",
    "ax1.bar(1+np.arange(N), pca.explained_variance_ratio_, color=color)\n",
    "ax1.set_xticks(1+np.arange(N, step=2))\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylabel(\"Explained variance ratio\", color=color)\n",
    "ax1.set_xlabel(\"Generated feature\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.plot(1+np.arange(N), np.cumsum(pca.explained_variance_ratio_), color=color)\n",
    "ax2.set_ylabel(\"Cumulative explained variance ratio\", color=color)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79c9ff215d72a4ecf7a25f93d9c1df33",
     "grade": false,
     "grade_id": "cell-3705c56d8bffebb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If we choose to use 784 components, then the cumulative explained variance ratio will be 1, i.e., no data would be lost. This corresponds to projecting the data to the same space where it originally \"lived\", i.e. not doing anything. Note, however, that the features will likely still change, even though no data is lost. What happens is essentially a change of basis of the feature space. \n",
    "\n",
    "Of course, there's not much use in keeping all the features, so how many features (components) should we keep? There's no clear cut answer to that here and there rarely is. In this case, let's keep 15 components, because the explained variance ratio drops to under 2% per feature around that point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c31b57da87ccaaf11531ec03dadc7487",
     "grade": false,
     "grade_id": "cell-c14e74309f2c749d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "N = 15\n",
    "pca.set_params(n_components=N)\n",
    "X_train_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c97c225bec07c489f7d3ad57e099e13f",
     "grade": false,
     "grade_id": "cell-e216dfc0cdd1a565",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's visually confirm that the compression hasn't affected the pictures too much. Based on the representatives chosen, it seems that keeping just 15/784 dimensions still leaves the digits legible, if slightly blurry.\n",
    "\n",
    "To view the image after dropping the less important features, we need to transform the 15-dimensional vector back into the original 784-dimensional space. For this, we'll use the ``inverse_transform`` method. Mathematically, all of the components form a basis of the original space, so this function creates a linear combination of the first 15 basis vector with coefficients corresponding to the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "129569a0c25c499e1f668303590ef92b",
     "grade": false,
     "grade_id": "cell-0ca3c0153315e4c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the digits with only 15 components\n",
    "plot_digits(pca.inverse_transform(X_train_reduced), y_train, title=\"An example of each class with 15 PCA components\")\n",
    "plt.show()\n",
    "\n",
    "# plot the same picture with different numebrs of features\n",
    "digit = 4\n",
    "cols = 8\n",
    "fig, axs = plt.subplots(2, cols, figsize=(12, 8))\n",
    "fig.suptitle(\"An image of a '4' with varying number of PCA components\", fontsize=14)\n",
    "idx = np.argwhere(y_train == str(digit))[0]\n",
    "for n in 1+np.arange(N):\n",
    "    X_nfeatures = X_train_reduced[idx]*[1 if i < n else 0 for i in range(N)]\n",
    "    # transform the data back into the original 784-dimensional space\n",
    "    im = pca.inverse_transform(X_nfeatures).reshape(28, 28)\n",
    "    # plot it\n",
    "    axs[(n-1)//cols, (n-1) % cols].set_title(f\"{(n)} feature{'s' if n>1 else ''}\")\n",
    "    axs[(n-1)//cols, (n-1) % cols].imshow(im, cmap='gray')\n",
    "    axs[(n-1)//cols, (n-1) % cols].axis('off')\n",
    "# plot the original\n",
    "axs[-1, -1].set_title(f'original')\n",
    "axs[-1, -1].imshow(X_train[idx].reshape(28, 28), cmap='gray')\n",
    "axs[-1, -1].axis('off')\n",
    "# adjust spacing\n",
    "fig.subplots_adjust(wspace=0.1, hspace=-0.8, top=1.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "342b6c93439cd969b871a3a3dd3b1886",
     "grade": false,
     "grade_id": "cell-e514166a2caf4b7e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "# Student Task A9.1\n",
    "Use the transformed data ``X_train_reduced`` to train a logistic regressor. Then compute its accuracy on the validation set. Do not forget what variables have already been transformed using PCA and which still need to have it applied. \n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d85437ff7b7712a7924a31ebc847c546",
     "grade": false,
     "grade_id": "cell-96bd8d7f3bd50285",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# clf_2 = # create the object, SET solver='sag'\n",
    "# clf_2. # fit the data\n",
    "# y_pred = # compute the prediction on the validating set\n",
    "# multi_accuracy = # compute the accuracy score\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(f\"Prediction accuracy: {100*multi_accuracy:.2f}%\")\n",
    "generate_confusion_matrix(y_val, y_pred)\n",
    "plt.show()\n",
    "\n",
    "assert isinstance(clf_2, LogisticRegression), \"You need to use Logistic regression!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8854f045d0fb0e8e997a2977f6fbc0b",
     "grade": true,
     "grade_id": "cell-c27f2e4038d7a667",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## test cell for A9.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cb948de58ce4791bc396e5322c2e0f2",
     "grade": true,
     "grade_id": "cell-6741090189f8bf0f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## test cell for A9.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c967f38bb3bad65bafdcb7f164afb0b3",
     "grade": false,
     "grade_id": "cell-8a88b5edc9f7a47c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Demo: Data visualization\n",
    "Another application of PCA is visualization of high-dimensional data. Here, we plot each image as a single point based on its coordinates in the 2D subspace generated by PCA. For example, we can see that 1's seem to be very easy to tell apart and they all look very similar, whereas these two dimensions are certainly not enough to tell apart 4's and 9's in any meaningful way. This confirms what we saw earlier - with only a few components, the 4 looked like a 9!\n",
    "\n",
    "Note that since the features carry no meaning anymore, we label the axes simply as PCA1 and PCA2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a90ba26e1d44daf57f600dcc4391913",
     "grade": false,
     "grade_id": "cell-5aa6ed771bfc23c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "# convert the labels to numbers, each will be assigned a separate color based on the cmap specified\n",
    "colors = [int(x) for x in y_train]\n",
    "sc = plt.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], s=1, c=colors, cmap='tab10')\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.legend(*sc.legend_elements(), title='digit')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06b0dcfecd28ec253d9015c898a946e2",
     "grade": false,
     "grade_id": "cell-b4ccacb336052077",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Lasso\n",
    "Another way of \"automatically\" lowering the number of features is known as Lasso. An advantage it has over PCA is that it takes the labels into account too. Furthermore, you do not lose intepretability of the resulting features: these will simply be a subset of the original features.\n",
    "\n",
    "The Lasso is actually just linear regression with L1 regularization (in the same way that that in A6 we used Ridge - L2 regularization). That means that the squared error loss is replaced by\n",
    "$$\n",
    "L((\\mathbf{x},y),h^{(\\mathbf{w})})=(y-\\mathbf{w}^T\\mathbf{x})^2+\\lambda\\|\\mathbf{w}\\|_1.\n",
    "$$\n",
    "By varying parameter $\\lambda$, we influence the number of features that will have zero weights in the resulting trained model.\n",
    "\n",
    "The reason why using L1 regularization would typically lead to sparse (\"many zeros\") solutions is not immediately clear. The animation below (courtesy of [Itay Evron](https://github.com/ievron/RegularizationAnimation/)) could give you an intuition (along with the description underneath):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6861a8fa996255b23d809702cd848a1",
     "grade": false,
     "grade_id": "cell-345442ac9303c305",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![Regularization](regularization.gif \"regularization\")\n",
    "\n",
    "*The unit \"circles\" (points with norm $\\leq1$) are shown in turquoise. Notice that in the 1-norm case (on the right), the unit circle is actually a square rotated by 45 degrees. Convince yourself this is indeed the case when*\n",
    "$$\n",
    "\\|(x,y)\\|_1=|x|+|y|.\n",
    "$$\n",
    "*In grey, you can see the optimal value of the loss function (the center of the ellipses), each of the ellipses marks a countour line (curve where the loss assumes the same value). As the loss function changes (this corresponds to changing the training data), you can watch how does the orange point, which is the optimum on the unit circle, behaves. You can notice that in the case of L1 regularization, it is much more often on one of the axes, meaning that one of the weights is zero.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9956cb8e557c88aa6abf486ad01bb113",
     "grade": false,
     "grade_id": "cell-cc09a5d221d19dcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Demo: Comparison of Lasso and Ridge\n",
    "Now let's study how Lasso compares to Ridge. The dataset we use for training is the same we used when we studied ridge regression originally (in A6): noisy sinusoidal data. We will use polynomial features of degree 10 and look at how the penalty coefficient affects the resulting model.\n",
    "\n",
    "The first plot compares how the error depends on parameter ``alpha`` and is not of very high importance here. In the plots that follow, you can observe how increasing the penalty weight, lasso tends to decrease the importance of features one at a time, while with ridge, this affects all features mostly similarly. Also notice that lasso makes the feature weights truly zero, this never truly happens with ridge, where they simply get very close to zero but never quite reach it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c1f40ed40e95a711fa01c5caebade18",
     "grade": false,
     "grade_id": "cell-04d16e5c9eec477b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "interval = [0, 7]\n",
    "\n",
    "\n",
    "def make_data(m=30):\n",
    "    # generate datapoints with a noisy sinusoidal relationship\n",
    "    np.random.seed(seed=1)\n",
    "    X = (interval[1]-interval[0])*np.random.rand(m, 1)+interval[0]\n",
    "    y = np.sin(X)+0.5*np.random.randn(m, 1)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_validate_poly(X_train, y_train, X_val, y_val, model, degree=10):\n",
    "    # train a polynomial model and validate it\n",
    "\n",
    "    # generate polynomial features\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    X_val_poly = poly_features.fit_transform(X_val)\n",
    "\n",
    "    # learn the model and validate it\n",
    "    scaler = StandardScaler().fit(X_train_poly)\n",
    "    reg = model().fit(scaler.transform(X_train_poly), y_train)\n",
    "    y_val_pred = reg.predict(scaler.transform(X_val_poly))\n",
    "    val_err = mean_squared_error(y_val, y_val_pred)\n",
    "    return reg, val_err\n",
    "\n",
    "\n",
    "X2, y2 = make_data(50)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2, y2, test_size=0.33, random_state=2)\n",
    "\n",
    "# iteratively try different lambdas\n",
    "l_range = (-5, 6)\n",
    "l1_errs = []\n",
    "l1_regs = []\n",
    "l2_errs = []\n",
    "l2_regs = []\n",
    "lambdas = np.logspace(*l_range, num=200)  # candidate lambdas\n",
    "for l in lambdas:\n",
    "    reg, val_err = train_validate_poly(\n",
    "        X2_train, y2_train, X2_val, y2_val, lambda: Lasso(alpha=l, max_iter=20000))\n",
    "    l1_regs.append(reg)\n",
    "    l1_errs.append(val_err)\n",
    "\n",
    "    reg, val_err = train_validate_poly(\n",
    "        X2_train, y2_train, X2_val, y2_val, lambda: Ridge(alpha=l))\n",
    "    l2_regs.append(reg)\n",
    "    l2_errs.append(val_err)\n",
    "\n",
    "# plot the data\n",
    "fig, axs = plt.subplots(4, 1, figsize=(12, 22))\n",
    "plt.setp(axs, xscale='log', xticks=[10**x for x in range(*l_range)], xlabel='lambda', ylabel='Mean Squared Error (MSE)')\n",
    "axs[0].plot(lambdas, l1_errs, color='red', label='l1 validation errror')\n",
    "axs[0].plot(lambdas, l2_errs, color='blue', label='l2 validation errror')\n",
    "axs[0].set_title('Effect of penalty coefficient on validation error')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title(\"Effect of penalty coefficient on number of negligible features\")\n",
    "axs[1].set_ylabel('Number negligible weights in $w$')\n",
    "axs[1].plot(lambdas, [sum(r.coef_ == 0) for r in l1_regs], color='red', label='l1 zero coefficients')\n",
    "axs[1].plot(lambdas, [sum(abs(r.coef_[0]) < 1e-5) for r in l2_regs], color='blue', label=f'l2 coefficients smaller than {1e-5:.1e}')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.setp(axs[2:], yscale='symlog')\n",
    "l1_weights = np.array([r.coef_ for r in l1_regs])\n",
    "l2_weights = np.array([r.coef_[0] for r in l2_regs])\n",
    "axs[2].set_title(\"Effect of penalty coefficient on feature weights in Lasso\")\n",
    "axs[2].plot(lambdas, l1_weights)\n",
    "axs[2].set_ylabel(\"Weight values\")\n",
    "axs[3].plot(lambdas, l2_weights)\n",
    "axs[3].set_title(\"Effect of penalty coefficient on feature weights in Ridge\")\n",
    "axs[3].set_ylabel(\"Weight values\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "269f863fbe53bc3bb03754fd4072815d",
     "grade": false,
     "grade_id": "cell-51fc0a782d7c3e91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "# Student Task A9.2\n",
    "Take a look at the plots above. Answer the following questions:\n",
    "* Q1: By looking at the plots above, it is certain that the model resulting from lasso with ``alpha=1e-2`` is a degree 10 polynomial in ``x``.\n",
    "    * ``A1=0``: disagree, ``A1=1``: agree.\n",
    "* Q2: By looking at the plots above, it is certain that the model resulting from ridge with ``alpha=1e-2`` is a degree 10 polynomial in ``x``.\n",
    "    * ``A2=0``: disagree, ``A2=1``: agree.\n",
    "* Q3: What is the reason the validation MSE error is similar for high ``alpha`` (see the first plot) for Lasso and Ridge?\n",
    "    * ``A3=0``: Both models approach the same function as ``alpha`` goes to infinity.\n",
    "    * ``A3=1``: This is just a coincidence.\n",
    "    * ``A3=2``: For high values of ``alpha``, the norms converge.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02c6d139a156b1e4606ff63062c42a92",
     "grade": false,
     "grade_id": "cell-38618a9f3ce24f8e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# A1 = # 0 or 1\n",
    "# A2 = # 0 or 1\n",
    "# A3 = # 0, 1 or 2\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "assert A1 in {0, 1}\n",
    "assert A2 in {0, 1}\n",
    "assert A3 in {0, 1, 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58626be74d952cf50c04201eb5240232",
     "grade": true,
     "grade_id": "cell-62ec7864568c6b1d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## test cell for A9.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4da2989f63f39545fc353d744dc93586",
     "grade": true,
     "grade_id": "cell-021f638d1592d83c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## test cell for A9.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16cf9b4f23db3692ab2abb42d1c2bc4f",
     "grade": true,
     "grade_id": "cell-9432539f3d3bb3c4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## test cell for A9.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3064141ccf1b091fe6da17a956306be",
     "grade": false,
     "grade_id": "cell-408f8de4be115489",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Demo: PCA ignores the labels\n",
    "Lasso can also help us when PCA fails. To show this, we'll create an artificial dataset with two randomly generated features with significantly different scales (=variances). The labels will depend on the feature with lower variance. PCA, which ignores the labels, will then do what it's supposed to: find a 1D subspace that fits the data the best, which, in this case, roughly means keeping the irrelevant feature. This is because in some sense, PCA measures the importance by variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37dc111a58c312480ae06737653dbfb8",
     "grade": false,
     "grade_id": "cell-a598919bd9020eef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_dataset(n=200, f1_scale=1, f2_scale=0.1, label_noise_scale=0.01):\n",
    "    np.random.seed(seed=42)\n",
    "    # generate the two features\n",
    "    feature1 = np.random.normal(scale=f1_scale, size=(n, 1))\n",
    "    feature2 = np.random.normal(scale=f2_scale, size=(n, 1))\n",
    "    X = np.concatenate((feature1, feature2), axis=1)\n",
    "    # generate the labels by adding noise to feature 2\n",
    "    y = feature2+np.random.normal(scale=label_noise_scale, size=(n, 1))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X3, y3 = generate_dataset()\n",
    "# visualize the dataset\n",
    "sc = plt.scatter(X3[:, 0], X3[:, 1], s=15, c=[y3])\n",
    "cbar=plt.colorbar()\n",
    "cbar.set_label(\"label value\")\n",
    "plt.axis('equal')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "\n",
    "# do PCA\n",
    "X3_train, X3_val, y3_train, y3_val = train_test_split(X3, y3, test_size=0.33, random_state=42)\n",
    "pca = PCA(n_components=1).fit(X3_train)\n",
    "X3_1D_train = pca.transform(X3_train)\n",
    "\n",
    "# plot label wrt. feature 2\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axs[0].scatter(X3_train[:, 1], y3_train)\n",
    "axs[0].set_xlabel(\"Feature 2\")\n",
    "axs[0].set_ylabel(\"label\")\n",
    "axs[0].set_aspect('equal', 'box')\n",
    "\n",
    "# plot label wrt. generated feature\n",
    "axs[1].scatter(X3_1D_train, y3_train)\n",
    "axs[1].set_xlabel(\"PCA1\")\n",
    "axs[1].set_ylabel(\"label\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5fa7e134bfd6423d600f5ed46f827c9",
     "grade": false,
     "grade_id": "cell-7fe47608aa2b6848",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "# Student Task A9.3\n",
    "Find a value of ``alpha`` so that precisely one of the coefficients is nonzero after training on this dataset.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0ec9ed562af043413a53330fb99a545",
     "grade": false,
     "grade_id": "cell-0a8136b23935ce4c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# could be a task for students: find alpha such that lasso has precisely one nonzero weight\n",
    "# alpha = # set alpha\n",
    "# lasso = # create a Lasso object with alpha as the parameter\n",
    "# lasso. # fit X3_train and y3_train\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(f\"Coefficients are {lasso.coef_}\")\n",
    "\n",
    "print(f\"Prediction MSE: {mean_squared_error(lasso.predict(X3_val), y3_val):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f636bf0f97a430ec56ee703589fdcb9c",
     "grade": true,
     "grade_id": "cell-c7a85caf63da0f14",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## test cell for A9.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "774c55ac769e5085845b2eb81bf8199f",
     "grade": true,
     "grade_id": "cell-c34073aaab104a7f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## test cell for A9.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba080f3217dd74375afff593152b1292",
     "grade": false,
     "grade_id": "cell-cc6cbb73899b71d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Demo: Recursive feature elimination\n",
    "There are also many methods for [feature selection](https://scikit-learn.org/stable/modules/feature_selection.html) already implemented in ``sklearn``. Here, we will take a quick look at Recursive feature elimination (RFE).\n",
    "\n",
    "Given an external estimator (e.g. Logistic Regression), RFE iteratively drops the \"least important\" feature(s) after fit. In the case of Logistic Regression, the feature(s) dropped will be the feature(s) corresponding to the weight(s) that has (have) the smallest absolute value. \n",
    "\n",
    "Below, you can see that according to RFE, the most important pixels seem to be those in the middle of the image, which is rather unsurprising - in this case, this is clear to a human that this would be the case. This approach can thus be useful when we do not have a good understanding of the relationships between the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87b55fd46698fe5b38ad7cefb6364ee3",
     "grade": false,
     "grade_id": "cell-8cb072fd1f570bea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "estimator=LogisticRegression(solver='liblinear')\n",
    "rfe=RFE(estimator,n_features_to_select=1,step=5)\n",
    "rfe.fit(X_MNIST[:100],y_MNIST[:100])\n",
    "\n",
    "ranking = rfe.ranking_.reshape((28,28))\n",
    "\n",
    "# plot pixel ranking\n",
    "plt.matshow(ranking, cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "plt.title(\"Ranking of pixels with RFE\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e24d5f07750e7916c6ab0f211acb0d360d24d1ad369135dc95bd82b4bfc72b8c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
